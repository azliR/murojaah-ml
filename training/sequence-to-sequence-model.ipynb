{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-07T02:06:57.025957859Z",
     "start_time": "2023-06-07T02:06:56.984684593Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/one-hot.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 29\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39msqueeze()[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Load every one-hot-encoded output as a dictionary\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../data/one-hot.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m one_hot_quran_pickle_file:\n\u001B[1;32m     30\u001B[0m     one_hot_obj \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(one_hot_quran_pickle_file)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_one_hot_encoded_verse\u001B[39m(surah_num, ayah_num):\n",
      "File \u001B[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:282\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/one-hot.pkl'"
     ]
    }
   ],
   "source": [
    "def convert_list_of_arrays_to_padded_array(list_varying_sizes, pad_value=0):\n",
    "    '''\n",
    "    Converts a list of arrays of varying sizes to a single numpy array. The extra elements are set to 0\n",
    "    '''\n",
    "    max_shape = [0] * len(list_varying_sizes[0].shape)\n",
    "    # first pass to compute the max size\n",
    "    for arr in list_varying_sizes:\n",
    "        shape = arr.shape\n",
    "        max_shape = [max(s1, s2) for s1, s2 in zip(shape, max_shape)]\n",
    "    padded_array = pad_value * np.ones((len(list_varying_sizes), *max_shape))\n",
    "\n",
    "    # second pass to fill in the values in the array:\n",
    "    for a, arr in enumerate(list_varying_sizes):\n",
    "        r, c = arr.shape\n",
    "        padded_array[a, :r, :c] = arr\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "\n",
    "def preprocess_encoder_input(arr):\n",
    "    '''\n",
    "    Simple method to handle the complex MFCC coefs that are produced during preprocessing. This means:\n",
    "    1. (For now), discarding one of the channels of the MFCC coefs\n",
    "    2. Collapsing any empty dimensions\n",
    "    '''\n",
    "    return arr.squeeze()[0]\n",
    "\n",
    "\n",
    "# Load every one-hot-encoded output as a dictionary\n",
    "with open('../data/one-hot.pkl', 'rb') as one_hot_quran_pickle_file:\n",
    "    one_hot_obj = pickle.load(one_hot_quran_pickle_file)\n",
    "\n",
    "\n",
    "def get_one_hot_encoded_verse(surah_num, ayah_num):\n",
    "    '''\n",
    "    Converts a one-hot-encoded verse into forms that can be used by the LSTM decoder\n",
    "\n",
    "    :param surah_num: an int designating the chapter number, one-indexed\n",
    "    :param ayah_num: an int designating the verse number, one-indexed\n",
    "    '''\n",
    "    # Load the preprocessed one-hot encoding\n",
    "    one_hot_verse = one_hot_obj['quran']['surahs'][surah_num - 1]['ayahs'][ayah_num - 1]['text']\n",
    "    num_chars_in_verse, num_unique_chars = one_hot_verse.shape\n",
    "\n",
    "    # Generate decoder_input_data\n",
    "    decoder_input = np.zeros((num_chars_in_verse + 2, num_unique_chars + 2))\n",
    "    decoder_input[0, :] = [0] * num_unique_chars + [1, 0]  # START token\n",
    "    decoder_input[1:num_chars_in_verse + 1, :-2] = one_hot_verse  # original verse\n",
    "    decoder_input[-1, :] = [0] * num_unique_chars + [0, 1]  # STOP token\n",
    "\n",
    "    # Generate decoder_target_data\n",
    "    decoder_target = np.zeros((num_chars_in_verse + 2, num_unique_chars + 2))\n",
    "    decoder_target[:num_chars_in_verse, :-2] = one_hot_verse  # original verse\n",
    "    decoder_target[-2, :] = [0] * num_unique_chars + [0, 1]  # STOP token\n",
    "\n",
    "    return decoder_input, decoder_target\n",
    "\n",
    "\n",
    "def build_dataset(local_coefs_dir='../.outputs/mfcc', surahs=[1], n=100):\n",
    "    '''\n",
    "    Builds a dataset to be used with the sequence-to-sequence network.\n",
    "\n",
    "    :param local_coefs_dir: a string with the path of the coefficients for prediction\n",
    "    '''\n",
    "\n",
    "    def get_encoder_and_decoder_data(n=100):\n",
    "        count = 0\n",
    "        encoder_input_data = []\n",
    "        decoder_input_data = []\n",
    "        decoder_target_data = []\n",
    "        for surah_num in surahs:\n",
    "            local_surah_dir = os.path.join(local_coefs_dir, \"s\" + str(surah_num))\n",
    "            for _, ayah_directories, _ in os.walk(local_surah_dir):\n",
    "                for ayah_directory in ayah_directories:\n",
    "                    ayah_num = ayah_directory[1:]\n",
    "                    local_ayah_dir = os.path.join(local_surah_dir, ayah_directory)\n",
    "                    for _, _, recording_filenames in os.walk(local_ayah_dir):\n",
    "                        for recording_filename in recording_filenames:\n",
    "                            local_coefs_path = os.path.join(local_ayah_dir, recording_filename)\n",
    "                            encoder_input = np.load(local_coefs_path)\n",
    "                            encoder_input = preprocess_encoder_input(encoder_input)\n",
    "                            encoder_input_data.append(encoder_input)\n",
    "\n",
    "                            decoder_input, decoder_target = get_one_hot_encoded_verse(int(surah_num), int(ayah_num))\n",
    "                            decoder_input_data.append(decoder_input)\n",
    "                            decoder_target_data.append(decoder_target)\n",
    "                            count += 1\n",
    "                            if count == n:\n",
    "                                return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "        return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "    encoder_input_data, decoder_input_data, decoder_target_data = get_encoder_and_decoder_data(n=n)\n",
    "    encoder_input_data = convert_list_of_arrays_to_padded_array(encoder_input_data)\n",
    "    decoder_input_data = convert_list_of_arrays_to_padded_array(decoder_input_data)\n",
    "    decoder_target_data = convert_list_of_arrays_to_padded_array(decoder_target_data)\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T02:18:54.508989274Z",
     "start_time": "2023-06-07T02:18:54.368536908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m latent_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m  \u001B[38;5;66;03m# Latent dimensionality of the encoding space.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m----> 6\u001B[0m encoder_input_data, decoder_input_data, decoder_target_data \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_dataset\u001B[49m(n\u001B[38;5;241m=\u001B[39mn)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'build_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 10  # Batch size for training.\n",
    "epochs = 25  # Number of epochs to train for.\n",
    "latent_dim = 10  # Latent dimensionality of the encoding space.\n",
    "n = 100\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = build_dataset(n=n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T02:07:24.656295053Z",
     "start_time": "2023-06-07T02:07:24.354474122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
